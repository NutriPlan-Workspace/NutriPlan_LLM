{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5e13d61",
    "outputId": "36029716-7da9-44f8-a1d4-083476261914"
   },
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/LLM_Nutriplan"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install langchain langchain-community transformers sentence-transformers pymongo"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ZAnC9Rj8lw4",
    "outputId": "f40f1904-abe7-43ae-d88d-dab201880146"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "import torch\n",
    "from typing import List, Dict, Tuple\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from pymongo import MongoClient\n",
    "import numpy as np"
   ],
   "metadata": {
    "id": "yxe2IUN78kBa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# B∆Ø·ªöC 1: HIERARCHICAL PARSER - Ph√¢n t√≠ch c·∫•u tr√∫c t√†i li·ªáu"
   ],
   "metadata": {
    "id": "ao4om93U5JAs"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRoyKvfr3Vpc"
   },
   "outputs": [],
   "source": [
    "class NutriPlanManualParser:\n",
    "    \"\"\"\n",
    "    Parser chuy√™n bi·ªát cho c·∫•u tr√∫c Nutri Plan Manual:\n",
    "    - Part 1, 2, 3, 4 (## headers)\n",
    "    - Subsections v·ªõi ### (numbered: 1., 2., 3.)\n",
    "    - Sub-subsections v·ªõi #### (Feature:, Capabilities:, etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            self.content = f.read()\n",
    "        self.sections = []\n",
    "\n",
    "    def parse(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Ph√¢n t√≠ch c·∫•u tr√∫c ph√¢n c·∫•p c·ªßa manual\n",
    "        Returns: List of sections v·ªõi metadata ƒë·∫ßy ƒë·ªß\n",
    "        \"\"\"\n",
    "        lines = self.content.split('\\n')\n",
    "        sections = []\n",
    "        current_section = None\n",
    "        buffer = []\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            # Detect headers (##, ###, ####)\n",
    "            header_match = re.match(r'^(#{2,4})\\s+(.+)$', line.strip())\n",
    "\n",
    "            if header_match:\n",
    "                # L∆∞u section tr∆∞·ªõc ƒë√≥\n",
    "                if current_section:\n",
    "                    current_section['content'] = '\\n'.join(buffer).strip()\n",
    "                    current_section['line_end'] = i - 1\n",
    "                    if current_section['content']:  # Ch·ªâ l∆∞u n·∫øu c√≥ content\n",
    "                        sections.append(current_section)\n",
    "\n",
    "                # Parse header m·ªõi\n",
    "                level = len(header_match.group(1))\n",
    "                title = header_match.group(2).strip()\n",
    "\n",
    "                # Detect section type\n",
    "                section_type = self._detect_section_type(title, level)\n",
    "\n",
    "                # T·∫°o section m·ªõi\n",
    "                current_section = {\n",
    "                    'level': level,\n",
    "                    'title': title,\n",
    "                    'type': section_type,\n",
    "                    'line_start': i,\n",
    "                    'line_end': None,\n",
    "                    'content': ''\n",
    "                }\n",
    "                buffer = []\n",
    "\n",
    "            elif current_section:\n",
    "                # Th√™m content v√†o buffer\n",
    "                buffer.append(line)\n",
    "\n",
    "        # L∆∞u section cu·ªëi c√πng\n",
    "        if current_section:\n",
    "            current_section['content'] = '\\n'.join(buffer).strip()\n",
    "            current_section['line_end'] = len(lines) - 1\n",
    "            if current_section['content']:\n",
    "                sections.append(current_section)\n",
    "\n",
    "        return sections\n",
    "\n",
    "    def _detect_section_type(self, title: str, level: int) -> str:\n",
    "        \"\"\"Ph√¢n lo·∫°i section type d·ª±a tr√™n pattern\"\"\"\n",
    "        title_lower = title.lower()\n",
    "\n",
    "        # Part-level (## Part X:)\n",
    "        if level == 2 and 'part' in title_lower:\n",
    "            return 'part'\n",
    "\n",
    "        # Feature categories\n",
    "        if 'hub' in title_lower:\n",
    "            return 'hub'\n",
    "\n",
    "        if any(word in title_lower for word in ['feature:', 'planner', 'collection', 'user']):\n",
    "            return 'feature'\n",
    "\n",
    "        # Subsections\n",
    "        if any(word in title_lower for word in ['how to', 'capabilities', 'limitations', 'usage']):\n",
    "            return 'instruction'\n",
    "\n",
    "        # Default\n",
    "        return 'general'\n",
    "\n",
    "    def build_hierarchy(self, sections: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        X√¢y d·ª±ng parent-child relationships v√† breadcrumb path\n",
    "        \"\"\"\n",
    "        stack = []\n",
    "\n",
    "        for section in sections:\n",
    "            # Pop sections c√≥ level >= current (kh√¥ng ph·∫£i parent)\n",
    "            while stack and stack[-1]['level'] >= section['level']:\n",
    "                stack.pop()\n",
    "\n",
    "            # Build path hierarchy\n",
    "            parent_titles = [s['title'] for s in stack]\n",
    "            section['parent_path'] = ' > '.join(parent_titles) if parent_titles else ''\n",
    "            section['full_path'] = (\n",
    "                section['parent_path'] + ' > ' if section['parent_path'] else ''\n",
    "            ) + section['title']\n",
    "\n",
    "            # Th√™m parent type ƒë·ªÉ filter t·ªët h∆°n\n",
    "            section['parent_type'] = stack[-1]['type'] if stack else None\n",
    "\n",
    "            stack.append(section)\n",
    "\n",
    "        return sections"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# B∆Ø·ªöC 2: SEMANTIC CHUNKER - Chia nh·ªè d·ª±a tr√™n ng·ªØ nghƒ©a"
   ],
   "metadata": {
    "id": "jQHbnq5R5MSq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SmartSemanticChunker:\n",
    "    \"\"\"\n",
    "    Chunker th√¥ng minh v·ªõi rules ƒë·∫∑c bi·ªát cho technical documentation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 chunk_size: int = 800,\n",
    "                 chunk_overlap: int = 200):\n",
    "\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "        # Separators ∆∞u ti√™n cho technical docs\n",
    "        self.separators = [\n",
    "            \"\\n\\n\\n\",          # Major section breaks\n",
    "            \"\\n\\n\",            # Paragraph breaks\n",
    "            \"\\n* \",            # Bullet points\n",
    "            \"\\n- \",            # Dash lists\n",
    "            \"\\n1. \", \"\\n2. \",  # Numbered lists\n",
    "            \". \",              # Sentences\n",
    "            \", \",              # Clauses\n",
    "            \" \",               # Words\n",
    "            \"\"                 # Characters\n",
    "        ]\n",
    "\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            separators=self.separators,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False\n",
    "        )\n",
    "\n",
    "    def chunk_section(self, section: Dict) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Chia section th√†nh chunks v·ªõi metadata enrichment\n",
    "        \"\"\"\n",
    "        content = section['content'].strip()\n",
    "        if not content:\n",
    "            return []\n",
    "\n",
    "        # Preprocessing: Gi·ªØ structure c·ªßa lists\n",
    "        content = self._preserve_list_structure(content)\n",
    "\n",
    "        # Split th√†nh chunks\n",
    "        texts = self.splitter.split_text(content)\n",
    "\n",
    "        # T·∫°o Documents v·ªõi rich metadata\n",
    "        docs = []\n",
    "        for i, text in enumerate(texts):\n",
    "            # Extract key phrases t·ª´ content\n",
    "            key_phrases = self._extract_key_phrases(text)\n",
    "\n",
    "            doc = Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    # Section info\n",
    "                    'section_title': section['title'],\n",
    "                    'section_level': section['level'],\n",
    "                    'section_type': section['type'],\n",
    "\n",
    "                    # Hierarchy\n",
    "                    'parent_path': section['parent_path'],\n",
    "                    'parent_type': section['parent_type'],\n",
    "                    'full_path': section['full_path'],\n",
    "\n",
    "                    # Chunk info\n",
    "                    'chunk_index': i,\n",
    "                    'total_chunks': len(texts),\n",
    "                    'chunk_size': len(text),\n",
    "\n",
    "                    # Searchability\n",
    "                    'key_phrases': key_phrases,\n",
    "\n",
    "                    # Source\n",
    "                    'source': 'nutri_plan_manual',\n",
    "                    'doc_type': 'user_manual'\n",
    "                }\n",
    "            )\n",
    "            docs.append(doc)\n",
    "\n",
    "        return docs\n",
    "\n",
    "    def _preserve_list_structure(self, content: str) -> str:\n",
    "        \"\"\"ƒê·∫£m b·∫£o bullet points kh√¥ng b·ªã t√°ch r·ªùi context\"\"\"\n",
    "        # Gi·ªØ nguy√™n formatting c·ªßa markdown lists\n",
    "        return content\n",
    "\n",
    "    def _extract_key_phrases(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract key terms ƒë·ªÉ enhance searchability\"\"\"\n",
    "        # Extract words in **bold** (markdown)\n",
    "        bold_phrases = re.findall(r'\\*\\*(.+?)\\*\\*', text)\n",
    "\n",
    "        # Extract quoted terms\n",
    "        quoted = re.findall(r'\"(.+?)\"', text)\n",
    "\n",
    "        # Extract code/path references\n",
    "        code_refs = re.findall(r'`(.+?)`', text)\n",
    "\n",
    "        # Combine v√† deduplicate\n",
    "        key_phrases = list(set(bold_phrases + quoted + code_refs))\n",
    "\n",
    "        return key_phrases[:10]  # Limit to top 10"
   ],
   "metadata": {
    "id": "XzOhuEKH5HhP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# B∆Ø·ªöC 3: EMBEDDING & STORAGE PIPELINE"
   ],
   "metadata": {
    "id": "oulmVmQ15Oil"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class DocumentEmbeddingPipeline:\n",
    "    \"\"\"Production-ready pipeline v·ªõi MongoDB Atlas Vector Search\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 mongodb_uri: str,\n",
    "                 database_name: str,\n",
    "                 collection_name: str = \"user_manual_embeddings\"):\n",
    "\n",
    "        print(\"üöÄ Initializing Embedding Pipeline...\")\n",
    "\n",
    "        # Initialize embeddings\n",
    "        print(\"   Loading multilingual-e5-small model...\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"intfloat/multilingual-e5-small\",\n",
    "            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        print(f\"   ‚úì Model loaded on: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "        # MongoDB connection\n",
    "        print(\"   Connecting to MongoDB...\")\n",
    "        self.client = MongoClient(mongodb_uri)\n",
    "        self.db = self.client[database_name]\n",
    "        self.collection = self.db[collection_name]\n",
    "        print(f\"   ‚úì Connected to {database_name}.{collection_name}\")\n",
    "\n",
    "        # Setup indexes\n",
    "        self._create_indexes()\n",
    "\n",
    "    def _create_indexes(self):\n",
    "        \"\"\"T·∫°o indexes cho efficient querying\"\"\"\n",
    "        print(\"\\nüìä Setting up database indexes...\")\n",
    "\n",
    "        # Text search indexes\n",
    "        try:\n",
    "            self.collection.create_index([(\"metadata.section_title\", \"text\")])\n",
    "            self.collection.create_index([(\"metadata.key_phrases\", \"text\")])\n",
    "            print(\"   ‚úì Text search indexes created\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Text index warning: {e}\")\n",
    "\n",
    "        # Metadata filter indexes\n",
    "        indexes = [\n",
    "            \"metadata.full_path\",\n",
    "            \"metadata.section_type\",\n",
    "            \"metadata.section_level\",\n",
    "            \"metadata.parent_type\"\n",
    "        ]\n",
    "\n",
    "        for idx in indexes:\n",
    "            self.collection.create_index(idx)\n",
    "\n",
    "        print(\"   ‚úì Metadata filter indexes created\")\n",
    "\n",
    "        # Vector Search reminder\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚ö†Ô∏è  ATLAS VECTOR SEARCH INDEX REQUIRED FOR PRODUCTION\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Database: {self.db.name}\")\n",
    "        print(f\"Collection: {self.collection.name}\")\n",
    "        print(\"\\nCreate index with this configuration:\")\n",
    "        print(\"\"\"\n",
    "{\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"type\": \"vector\",\n",
    "      \"path\": \"embedding\",\n",
    "      \"numDimensions\": 384,\n",
    "      \"similarity\": \"cosine\"\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"filter\",\n",
    "      \"path\": \"metadata.section_type\"\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"filter\",\n",
    "      \"path\": \"metadata.parent_type\"\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"filter\",\n",
    "      \"path\": \"metadata.full_path\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "        \"\"\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    def process_document(self, filepath: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Main processing pipeline\n",
    "\n",
    "        Args:\n",
    "            filepath: Path to the markdown manual file\n",
    "\n",
    "        Returns:\n",
    "            Statistics dictionary\n",
    "        \"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"üîÑ DOCUMENT PROCESSING PIPELINE\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        # Step 1: Parse document\n",
    "        print(\"\\nüìñ Step 1: Parsing document structure...\")\n",
    "        parser = NutriPlanManualParser(filepath)\n",
    "        sections = parser.parse()\n",
    "        sections = parser.build_hierarchy(sections)\n",
    "\n",
    "        print(f\"   ‚úì Parsed {len(sections)} sections\")\n",
    "\n",
    "        # Show section distribution\n",
    "        section_types = {}\n",
    "        for s in sections:\n",
    "            section_types[s['type']] = section_types.get(s['type'], 0) + 1\n",
    "\n",
    "        print(\"\\n   Section distribution:\")\n",
    "        for stype, count in section_types.items():\n",
    "            print(f\"   - {stype}: {count}\")\n",
    "\n",
    "        # Step 2: Chunk sections\n",
    "        print(\"\\n‚úÇÔ∏è  Step 2: Creating semantic chunks...\")\n",
    "        chunker = SmartSemanticChunker(chunk_size=800, chunk_overlap=200)\n",
    "        all_docs = []\n",
    "\n",
    "        for section in sections:\n",
    "            docs = chunker.chunk_section(section)\n",
    "            all_docs.extend(docs)\n",
    "\n",
    "        print(f\"   ‚úì Created {len(all_docs)} chunks\")\n",
    "\n",
    "        # Step 3: Generate embeddings\n",
    "        print(\"\\nüßÆ Step 3: Generating embeddings...\")\n",
    "        texts = [doc.page_content for doc in all_docs]\n",
    "\n",
    "        print(f\"   Processing {len(texts)} texts...\")\n",
    "        vectors = self.embeddings.embed_documents(texts)\n",
    "\n",
    "        print(f\"   ‚úì Generated {len(vectors)} vectors (dim={len(vectors[0])})\")\n",
    "\n",
    "        # Step 4: Store in MongoDB\n",
    "        print(\"\\nüíæ Step 4: Storing in MongoDB...\")\n",
    "\n",
    "        # Clear existing data\n",
    "        delete_result = self.collection.delete_many({})\n",
    "        print(f\"   Cleared {delete_result.deleted_count} existing documents\")\n",
    "\n",
    "        # Prepare documents\n",
    "        documents_to_insert = []\n",
    "        for doc, vector in zip(all_docs, vectors):\n",
    "            doc_dict = {\n",
    "                'text': doc.page_content,\n",
    "                'embedding': vector,\n",
    "                'metadata': doc.metadata\n",
    "            }\n",
    "            documents_to_insert.append(doc_dict)\n",
    "\n",
    "        # Batch insert\n",
    "        result = self.collection.insert_many(documents_to_insert)\n",
    "        print(f\"   ‚úì Inserted {len(result.inserted_ids)} documents\")\n",
    "\n",
    "        # Return statistics\n",
    "        stats = {\n",
    "            'total_sections': len(sections),\n",
    "            'section_types': section_types,\n",
    "            'total_chunks': len(all_docs),\n",
    "            'total_embeddings': len(vectors),\n",
    "            'embedding_dim': len(vectors[0]),\n",
    "            'inserted_documents': len(result.inserted_ids)\n",
    "        }\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚úÖ PROCESSING COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def semantic_search(self,\n",
    "                       query: str,\n",
    "                       top_k: int = 5,\n",
    "                       filters: Dict = None,\n",
    "                       use_atlas_search: bool = True) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        T√¨m ki·∫øm semantic v·ªõi optional filters\n",
    "\n",
    "        Args:\n",
    "            query: Search query\n",
    "            top_k: Number of results\n",
    "            filters: MongoDB filter dict (e.g., {\"metadata.section_type\": \"feature\"})\n",
    "            use_atlas_search: Use Atlas Vector Search or fallback\n",
    "        \"\"\"\n",
    "        # Embed query\n",
    "        query_vector = self.embeddings.embed_query(query)\n",
    "\n",
    "        if use_atlas_search:\n",
    "            try:\n",
    "                # Build aggregation pipeline\n",
    "                pipeline = [\n",
    "                    {\n",
    "                        \"$vectorSearch\": {\n",
    "                            \"index\": \"vector_index\",\n",
    "                            \"path\": \"embedding\",\n",
    "                            \"queryVector\": query_vector,\n",
    "                            \"numCandidates\": top_k * 10,\n",
    "                            \"limit\": top_k\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "                # Add filters if provided\n",
    "                if filters:\n",
    "                    pipeline.append({\"$match\": filters})\n",
    "\n",
    "                # Project results\n",
    "                pipeline.append({\n",
    "                    \"$project\": {\n",
    "                        \"text\": 1,\n",
    "                        \"metadata\": 1,\n",
    "                        \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "                    }\n",
    "                })\n",
    "\n",
    "                results = list(self.collection.aggregate(pipeline))\n",
    "\n",
    "                return [{\n",
    "                    'text': r['text'],\n",
    "                    'metadata': r['metadata'],\n",
    "                    'score': r['score']\n",
    "                } for r in results]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Atlas Vector Search failed: {e}\")\n",
    "                print(\"   Falling back to manual search...\")\n",
    "                use_atlas_search = False\n",
    "\n",
    "        # Fallback: Manual search\n",
    "        if not use_atlas_search:\n",
    "            query_dict = filters if filters else {}\n",
    "            all_docs = list(self.collection.find(query_dict))\n",
    "\n",
    "            similarities = []\n",
    "            for doc in all_docs:\n",
    "                similarity = np.dot(query_vector, doc['embedding'])\n",
    "                similarities.append({\n",
    "                    'text': doc['text'],\n",
    "                    'metadata': doc['metadata'],\n",
    "                    'score': float(similarity)\n",
    "                })\n",
    "\n",
    "            similarities.sort(key=lambda x: x['score'], reverse=True)\n",
    "            return similarities[:top_k]\n",
    "\n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"L·∫•y th·ªëng k√™ v·ªÅ database\"\"\"\n",
    "        total_docs = self.collection.count_documents({})\n",
    "\n",
    "        # Aggregate by section type\n",
    "        pipeline = [\n",
    "            {\n",
    "                \"$group\": {\n",
    "                    \"_id\": \"$metadata.section_type\",\n",
    "                    \"count\": {\"$sum\": 1}\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        type_distribution = {\n",
    "            item['_id']: item['count']\n",
    "            for item in self.collection.aggregate(pipeline)\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            'total_chunks': total_docs,\n",
    "            'type_distribution': type_distribution\n",
    "        }\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"ƒê√≥ng MongoDB connection\"\"\"\n",
    "        self.client.close()\n",
    "        print(\"‚úì MongoDB connection closed\")"
   ],
   "metadata": {
    "id": "5YrSbNbC5Pyg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# B∆Ø·ªöC 4: USAGE EXAMPLE"
   ],
   "metadata": {
    "id": "JGvn6n095ShT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"Example usage\"\"\"\n",
    "\n",
    "    # Configuration\n",
    "    MONGODB_URI = os.getenv(\"MONGODB_URI\")\n",
    "    DATABASE_NAME = \"test\"\n",
    "    COLLECTION_NAME = \"llm_documents\"\n",
    "    MANUAL_FILE = \"user_manual_for_rag.md\"  # Path to your manual file\n",
    "\n",
    "    # Initialize pipeline\n",
    "    pipeline = DocumentEmbeddingPipeline(\n",
    "        mongodb_uri=MONGODB_URI,\n",
    "        database_name=DATABASE_NAME,\n",
    "        collection_name=COLLECTION_NAME\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Process document\n",
    "        # stats = pipeline.process_document(MANUAL_FILE)\n",
    "\n",
    "        # Print statistics\n",
    "        # print(\"\\nüìä PROCESSING STATISTICS:\")\n",
    "        # print(f\"Total Sections: {stats['total_sections']}\")\n",
    "        # print(f\"Total Chunks: {stats['total_chunks']}\")\n",
    "        # print(f\"Embedding Dimension: {stats['embedding_dim']}\")\n",
    "\n",
    "        # Test searches\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üîç TESTING SEMANTIC SEARCH\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        test_queries = [\n",
    "            \"T√¥i ƒë√£ ƒëƒÉng nh·∫≠p, l√†m sao ƒë·ªÉ t√¥i t·∫°o th·ª±c ƒë∆°n trong ng√†y\",\n",
    "            \"What are the guest features?\",\n",
    "            \"How to log out of the application?\",\n",
    "            \"How does the grocery list work?\"\n",
    "        ]\n",
    "\n",
    "        for query in test_queries:\n",
    "            print(f\"\\n‚ùì Query: '{query}'\")\n",
    "            results = pipeline.semantic_search(query, top_k=3)\n",
    "\n",
    "            for i, result in enumerate(results, 1):\n",
    "                print(f\"\\n  [{i}] Score: {result['score']:.4f}\")\n",
    "                print(f\"      Path: {result['metadata']['full_path']}\")\n",
    "                print(f\"      Preview: {result['text']}\")\n",
    "\n",
    "        # Database stats\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        db_stats = pipeline.get_statistics()\n",
    "        print(\"üìà DATABASE STATISTICS:\")\n",
    "        print(f\"Total Chunks: {db_stats['total_chunks']}\")\n",
    "        print(\"Type Distribution:\", db_stats['type_distribution'])\n",
    "\n",
    "    finally:\n",
    "        pipeline.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "digemZcI5T-y",
    "outputId": "248e6018-6c8f-44ac-c0ad-82dac7312a8d"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}